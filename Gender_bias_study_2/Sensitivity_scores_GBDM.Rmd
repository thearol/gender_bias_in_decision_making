---
title: "sensitivity_scores_db"
author: "Thea Rolskov Sloth"
date: "11/10/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Setting working directory and loading data
```{r loading packages and data}
library(pacman)
p_load(plotly, jpeg, tm, brms, tidyverse, tidybayes, ggplot2, LaplacesDemon, rethinking, tidyr, reshape2, tibble, stringr, here, data.table, mousetrap, base)

#setting working directory
setwd("~/gender_bias_in_decision_making/Gender_bias_study_2")

#Loading data
data_db <- read.csv("data_GBDM.csv")[,2:40]
disagree_db <- read.csv("disagree_data_GBDM.csv")[,2:42]

```

```{r remove joint columns in data_db }
data_db <-  data_db %>% select(-contains('oint'))
data_db <- data_db[,1:30]

```


#Transforming to long format

```{r making long format, include = FALSE}

#create dataframe with only data from left player
left <- data_db %>% select(-contains('right'), c(Gender_right))
colnames(left)[colnames(left) == "Gender_right"] <- "Partner_gender"


#create dataframe with only data from right player
right <- data_db %>% select(-contains('left'), c(Gender_left))
colnames(right)[colnames(right) == "Gender_left"] <- "Partner_gender"


#rename columns to not contain left or right
names(left) <- gsub(pattern = "_left", replacement = "", x = names(left))
names(right) <- gsub(pattern = "_right", replacement = "", x = names(right))


#make same type of column in subject (factor)
right$SubjectID <- as.factor(right$SubjectID)
left$SubjectID <- as.factor(left$SubjectID)

#make column that specifies side
right$Side <- as.factor("right")
left$Side <- as.factor("left")


#Joining the dataframes
long_data_db <- rbind(left, right)

```

#Creating csv-file
```{r create csv file with all trials}

write.csv(long_data_db, file = "data_long_format_GBDM.csv")
```


###MODELLING




## Create models for sensitivity scores

```{r get priors, define priors and prior predictive check, eval = FALSE}

#use get prior function
get_prior(response_dummy ~ diff + (1 + diff|SubjectID_unique), family = "bernoulli", data = long_data_db)

#Defining priors
prior_sensitivity =  c(
  prior(normal(0,0.1), class = "b", coef = "diff"),
  prior(normal(0,0.2), class = "Intercept"),
  prior(normal(0,0.1), class = "sd", coef = "diff", group = "SubjectID_unique"),
  prior(normal(0,0.2), class = "sd", coef = "Intercept", group = "SubjectID_unique")
)


#Prior predictive check
prior_check_sensitivity <- brm(response_dummy ~ diff + (diff|SubjectID_unique), 
                               prior = prior_sensitivity,
                               data = long_data_db, 
                               sample_prior = "only",
                               iter = 4000, 
                               family = "bernoulli",
                               cores=2,
                               seed = 123,
                               chains=2)

pp_check(prior_check_sensitivity,nsamples = 100)

```


```{r sensitivity model, eval = FALSE}
#Making the model 
m_sensitivity <- brm(
  response_dummy ~ diff + (1+diff|SubjectID_unique),
  data = long_data_db,
  prior = prior_sensitivity,
  family = "bernoulli", #As we had a binary outcome, we set this to "bernoulli"
  seed = 123,# Adding a seed makes results reproducible.
  cores=4,
  chains=4,
  control = list(adapt_delta = 0.9)
) 

marginal_effects(m_sensitivity)
plot(m_sensitivity)

```

#Saving model estimates for sensitivity
```{r save the sensitivity estimates}
#Saving random effect for individual intercept
random_effects_intercept <- as.data.frame(ranef(m_sensitivity))[,1:2] 

#Saving random effect for individual slope
random_effects_slope <- as.data.frame(ranef(m_sensitivity))[,5:6]

#Saving fixed effects 
fixed_effects <- as.data.frame(fixef(m_sensitivity)) #Making data frame of fixed effects

#Make an empty dataframe
sensitivity_scores = data.frame(matrix(vector(), 128, 2,
                dimnames=list(c(), c("Intercept", "Slope"))),
                stringsAsFactors=F)

#Add fixed effects of intercept to random effects of intercept
sensitivity_scores$Intercept <- random_effects_intercept[,1] + fixed_effects[1,1]

#Add fixed effects of slope to random effects of slope
sensitivity_scores$Slope <- random_effects_slope[,1] + fixed_effects[2,1] #last year done differently

#Adding rownames
sensitivity_scores <- cbind(Row.Names = rownames(random_effects_intercept), as.data.frame(sensitivity_scores))

#Duplicating comlumn, but as a character so we can use it to merge later
sensitivity_scores$SubjectID_unique <- as.character(sensitivity_scores$Row.Names)

#Removing first column
sensitivity_scores <- sensitivity_scores[,2:4]

```

######## NOT YET A SENSITIVITY SCORES CSV ###########

#Spaghetti plot of all participants --> 
```{r}

#setting working directory


data <- read.csv("sensitivity_scores.csv", header = T, stringsAsFactors = F)

p <- ggplot(data = data.frame(x = c(-9, 9)),aes(x))

string <- "p"

for (row in 1:128) {
  intercept <- data[row,4]
  slope <- data[row,5]
  genderNumber <- as.numeric(str_extract(data[row,2], "\\d+"))
  genderColor <- ifelse(genderNumber %% 2 == 0, "orange", "cornflowerblue")
  
  string <- paste(string, " + stat_function(colour = '",genderColor,"', alpha = 0.4, fun = function(x) 1 / (1 + exp(-",slope,"*(x-",intercept,"))), n = 100)", sep="")
}

plotti <- eval(parse(text = string))

plotti + xlab ("Difficulty") + ylab("Propensity to answer 'right image'") + labs(title = "Individual sensitivity slopes") + theme(plot.title = element_text(hjust = 0.5))


#zoom for inspection
#plotti + coord_cartesian(xlim=c(-2,2), ylim = c(0.35,0.65)) #

```

#Merging the sensitivity scores with the rest of the data
```{r merge sensitivity scores with long_data_db, eval = FALSE}
long_data_db <- merge(long_data_db,sensitivity_scores, by="SubjectID_unique")
```


```{r creating new sensitivity score, eval = FALSE}

#Making a number of iterations variable
n = 1
s = 1
t = 10
k = 0

###Making a loop for individual slope
for (i in 1:96){ #Loop through number of trials
  print(i)
  if(i == 10){ #If the first 10
    for (p in unique(long_data_db$SubjectID_unique)){
      if (n == 1) {
        data_i <- filter(long_data_db, long_data_db$SubjectID_unique == p)[s:t, ]
        n = n + 1
        } else {
          data_i <- rbind(data_i, filter(long_data_db, long_data_db$SubjectID_unique == p)[s:t, ])
          n = n + 1
          } }
    
      #Defining priors
      prior_sensitivity =  c(
      prior(normal(0,0.1), class = "b", coef = "diff"),
      prior(normal(0,0.2), class = "Intercept"),
      prior(normal(0,0.1), class = "sd", coef = "diff", group = "SubjectID_unique"),
      prior(normal(0,0.2), class = "sd", coef = "Intercept", group = "SubjectID_unique")
    )
    
      #Making the model - using answer
      m_sensitivity <- brm(
      response_dummy ~ diff + (1+diff|SubjectID_unique),
      data = data_i,
      prior = prior_sensitivity,
      family = "bernoulli", #As we had a binary outcome, we set this to "bernoulli"
      seed = 123,# Adding a seed makes results reproducible.
      cores=2,
      chains=2,
      #control = list(adapt_delta = 0.9)
      )
    
    
    ###SAVING ESTIMATES
    #Saving random effect for individual slope
    sensitivity_scores_roll <- as.data.frame(ranef(m_sensitivity))[,5] + fixef(m_sensitivity)[2,1]
    
    #Adding rownames
    sensitivity_scores_roll <- cbind(Row.Names = rownames(as.data.frame(ranef(m_sensitivity))[,5:6]), as.data.frame(sensitivity_scores_roll))
    
    colnames(sensitivity_scores_roll) <- c("SubjectID_unique", i)
  }
    
    if(i > 10){
      k = k + 1
      n = 1
      s = s + 1
      t = t + 1
      for (r in unique(long_data_db$SubjectID_unique)){
        if (n == 1) {
          data_i <- filter(long_data_db, long_data_db$SubjectID_unique == r)[s:t, ]
          n = n + 1
        } else {
          data_i <- rbind(data_i, filter(long_data_db, long_data_db$SubjectID_unique == r)[s:t, ])
          n = n + 1
          } }
    
        #Defining priors
        prior_sensitivity =  c(
        prior(normal(0,0.1), class = "b", coef = "diff"),
        prior(normal(0,0.2), class = "Intercept"),
        prior(normal(0,0.1), class = "sd", coef = "diff", group = "SubjectID_unique"),
        prior(normal(0,0.2), class = "sd", coef = "Intercept", group = "SubjectID_unique")
      )
    
        #Making the model - using answer
        m_sensitivity <- brm(
        response_dummy ~ diff + (1+diff|SubjectID_unique),
        data = data_i,
        prior = prior_sensitivity,
        family = "bernoulli", #As we had a binary outcome, we set this to "bernoulli"
        seed = 123,# Adding a seed makes results reproducible.
        cores=2,
        chains=2,
        #control = list(adapt_delta = 0.9)
        )
        
    
    ###SAVING ESTIMATES
    
    #Saving random effect for individual slope
    sensitivity_scores_roll_temp <- as.data.frame(ranef(m_sensitivity))[,5] + fixef(m_sensitivity)[2,1]
    
    #Cbind new results to sensitivity score
    sensitivity_scores_roll <- cbind(sensitivity_scores_roll, sensitivity_scores_roll_temp)
    
    colnames(sensitivity_scores_roll)[k+2] <- i
    
} }
    
    

```



#Write a csv-file with the sensitivity scores with a rolling window
```{r write and read csv}
  
write.csv(sensitivity_scores_roll, file = "sensitivity_scores_rolling.csv")

#Loading the sensitivity scores again?
sensitivity_scores_roll <- read.csv("sensitivity_scores_rolling.csv")[,2:89]
```

#Merge the rolling window sensitivity scores with the existing data frame
```{r merge the new results with the long data frame}


#Creating a trial variable
n = 1
p = NULL
data_j = NULL
 for (p in unique(long_data_db$SubjectID_unique)){
   if (n == 1) {
     data_j <- filter(long_data_db, long_data_db$SubjectID_unique == p)
     data_j <- cbind(data_j, as.data.frame(1:sum(complete.cases(data_j$SubjectID))))
     colnames(data_j)[27] <- "Trial"
     n = n + 1
     } else {
     data_j <- rbind(data_j,mutate(filter(long_data_db, long_data_db$SubjectID_unique == p), Trial = 1:sum(complete.cases(filter(long_data_db, long_data_db$SubjectID_unique == p)$SubjectID))))
     n = n + 1
   }
 }

#Reshaping the data
mdata <- melt(sensitivity_scores_roll, id.vars=c("SubjectID_unique"), measure.vars = c(colnames(sensitivity_scores_roll)[2:88]), variable.name = "Trial", value.name = "Sensitivity_roll")

#Remove useless X in trial column
mdata$Trial <-str_replace_all(mdata$Trial, "X", "")

#Merge data_j with sensitivity scores
long_data_db <- merge(data_j, mdata, by = c("SubjectID_unique", "Trial"), all.x = TRUE, all.y = TRUE) #Insert NAs on trials lower than 10

```

#Plotting the model 

```{r slopes for individuals in single plots, eval = FALSE}
#Plot of individuals
marginal_effects(
  m_sensitivity,
  "diff", 
  conditions = distinct(long_data_db, SubjectID_unique),
  re_formula = NULL
  )
```

```{r marginal effects, classic, eval = FALSE }
#Summarized plot
marginal_effects(m_sensitivity)
```

#Transforming the data into wide format

```{r create dataframe that only contains the columns which we would like to merge into wide format}

#Make dataframe with intercept, slope, side, and unique subject
sensitivity_scores <- distinct(long_data_db, SubjectID_unique, Side, Intercept, Slope, Sensitivity_roll, Trial)

```


```{r make wide to merge with disagree data}
#Duplicating SubjectID to make into a character
sensitivity_scores$temp_SubjectID_unique <- as.character(sensitivity_scores$SubjectID_unique)

#deselect the old column
sensitivity_scores <- subset(sensitivity_scores, select = -c(SubjectID_unique))

#rename to the old name
names(sensitivity_scores)[names(sensitivity_scores) == "temp_SubjectID_unique"] <- "SubjectID_unique"


#Dividing into left and right data
sensitivity_scores_right <- filter(sensitivity_scores, Side == "right")
sensitivity_scores_left <- filter(sensitivity_scores, Side == "left")

#Deleting side column
sensitivity_scores_right <- subset(sensitivity_scores_right, select = -c(Side))
sensitivity_scores_left <- subset(sensitivity_scores_left, select = -c(Side))

#Renaming columns
colnames(sensitivity_scores_left) <-  c("Intercept_left", "Slope_left", "Sensitivity_roll_left", "Trial_left", "SubjectID_left_unique")
colnames(sensitivity_scores_right) <-  c("Intercept_right", "Slope_right", "Sensitivity_roll_right", "Trial_right", "SubjectID_right_unique")

#Creating a column for trial to the right
disagree_db$Trial_right <- disagree_db$Trial 

#Creating an identical column for trial to the left
colnames(disagree_db)[41] <- "Trial_left"

#Merging left data with sensitivity scores
disagree_db_wide <- merge(disagree_db, sensitivity_scores_left, by = c("SubjectID_left_unique", "Trial_left"))

#Merging right data with sensitivity scores
disagree_db_wide <- merge(disagree_db_wide, sensitivity_scores_right, by = c("SubjectID_right_unique", "Trial_right"))


```

#Excluding KK from the data 
```{r}
disagree_db_wide <- filter(disagree_db_wide, group_id != "25_KK_27" & group_id != "25_KK_28")
```


#Calculating skill difference

```{r calculating skill difference}
#Creating column of 0
disagree_db_wide$skill_dif <- 0 

disagree_db_wide$skill_dif <- ifelse(disagree_db_wide$chosen_leader == "Left_lead", disagree_db_wide$Slope_left/disagree_db_wide$Slope_right, disagree_db_wide$Slope_right/disagree_db_wide$Slope_left) 

#calculating skill difference as a ratio between leader and follower sensitivity, so that a skill difference of zero means no difference in skill
disagree_db_wide$skill_dif_c <- (disagree_db_wide$skill_dif)-1

```


#Skill difference rolling

```{r calculating skill difference rolling}
disagree_db_wide$skill_dif_roll <- 0 #Creating column of 0
disagree_db_wide$skill_dif_roll <- ifelse(disagree_db_wide$chosen_leader == "Left_lead", disagree_db_wide$Sensitivity_roll_left/disagree_db_wide$Sensitivity_roll_right, disagree_db_wide$Sensitivity_roll_right/disagree_db_wide$Sensitivity_roll_left) #calculating skill difference as a ratio between leader and follower sensitivity
disagree_db_wide$skill_dif_roll_c <- (disagree_db_wide$skill_dif_roll)-1

```



##Modelling confidence


#Confidence difference

```{r calculating confidence difference }
#Making the responses absolute numbers
disagree_db_wide$Response_left_abs<- abs(disagree_db_wide$Response_left)
disagree_db_wide$Response_right_abs<- abs(disagree_db_wide$Response_right)

#Left response scaled
disagree_db_wide <- scale_within(disagree_db_wide, variables = "Response_left_abs", within = "SubjectID_left_unique", center = TRUE, scale = TRUE, prefix = "z_")

#Right response scaled
disagree_db_wide <- scale_within(disagree_db_wide, variables = "Response_right_abs", within = "SubjectID_right_unique", center = TRUE, scale = TRUE, prefix = "z_")

#Creating a confidence ratio
disagree_db_wide$conf_dif <- 0 #Creating column of 0

#calculating confidence difference as a ratio between leader and follower confidence
disagree_db_wide$conf_dif <- ifelse(disagree_db_wide$chosen_leader == "Left_lead", disagree_db_wide$Response_left_abs/disagree_db_wide$Response_right_abs, disagree_db_wide$Response_right_abs/disagree_db_wide$Response_left_abs) 

#Centering so that a confidence ratio of 0 means that both are equally confident
disagree_db_wide$conf_dif_c <- (disagree_db_wide$conf_dif)-1

```



#Back to long format (to use in model)

```{r make long format}

#create dataframe with only data from left player
disagree_left <- disagree_db_wide %>% select(-contains('right'))

#make sure there is only the relevent information of the participant, thus, inserting NAs when the chosen leader was not left
disagree_left$skill_dif[disagree_left$chosen_leader != 'Left_lead'] <- NA 

disagree_left$conf_dif[disagree_left$chosen_leader != 'Left_lead'] <- NA 

disagree_left$skill_dif_c[disagree_left$chosen_leader != 'Left_lead'] <- NA 

disagree_left$conf_dif_c[disagree_left$chosen_leader != 'Left_lead'] <- NA 

disagree_left$leader_behavior[disagree_left$chosen_leader != 'Left_lead'] <- NA 

disagree_left$Leader_gender[disagree_left$chosen_leader != 'Left_lead'] <- NA 

disagree_left$Follower_gender[disagree_left$chosen_leader != 'Left_lead'] <- NA 



#create dataframe with only data from right player
disagree_right <- disagree_db_wide %>% select(-contains('left'))

#make sure there is only the relevent information of the participant, thus, inserting NAs when the chosen leader was not right
disagree_right$skill_dif[disagree_right$chosen_leader != 'Right_lead'] <- NA 

disagree_right$conf_dif[disagree_right$chosen_leader != 'Right_lead'] <- NA 

disagree_right$skill_dif_c[disagree_right$chosen_leader != 'Right_lead'] <- NA 

disagree_right$conf_dif_c[disagree_right$chosen_leader != 'Right_lead'] <- NA 

disagree_right$leader_behavior[disagree_right$chosen_leader != 'Right_lead'] <- NA 

disagree_right$Leader_gender[disagree_right$chosen_leader != 'Right_lead'] <- NA 

disagree_right$Follower_gender[disagree_right$chosen_leader != 'Right_lead'] <- NA 



#rename columns to not contain left or right
names(disagree_left) <- gsub(pattern = "_left", replacement = "", x = names(disagree_left))
names(disagree_right) <- gsub(pattern = "_right", replacement = "", x = names(disagree_right))


#make same type of column in subject
disagree_right$SubjectID <- as.factor(disagree_right$SubjectID)
disagree_left$SubjectID <- as.factor(disagree_left$SubjectID)

#make column that specifies side
disagree_right$Side <- as.factor("right")
disagree_left$Side <- as.factor("left")


#Joining the dataframes from both sides
disagree_db_long <- rbind(disagree_left, disagree_right)

#remove nas from data, so only containing leader information 
disagree_db_long <- na.omit(disagree_db_long)

```


#Dummy coding the leader behaviour variable
```{r dummy coding the leader bevhaiour}
#dummy coding leader behavior so that surrender = 1, and stick = 0
disagree_db_long$leader_behavior <- ifelse(disagree_db_long$leader_behavior == "surrender", 1, 0)
```


#Write CSV
```{r creating CSV}

write.csv(disagree_db_wide, file = "disagree_sensitivity_GBDM_wide_format.csv")
write.csv(disagree_db_long, file = "disagree_sensitivity_GBDM_long_format.csv")

```



